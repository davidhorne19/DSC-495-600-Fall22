{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Preprocessing: Cleaning Data](Preprocessing:-Cleaning-Data)\n",
    "\n",
    "1. [Import Data](#Import-Data)    \n",
    "2. [Breaking a Large String Into Smaller Strings](#Breaking-a-Large-String-Into-Smaller-Strings)   \n",
    "      a. [Individual Words](#Individual-Words)    \n",
    "      b. [Getting Word Counts](#Getting-Word-Counts)    \n",
    "      c. [Clear Limitations of Built-In `str` Methods](#Clear-Limitations-of-Built-In-`str`-Methods)\n",
    "3. [Conlclusions](Conclusions)      \n",
    "\n",
    "# Preprocessing: Cleaning Data\n",
    "\n",
    "There are numerous osteps that can be taken to help put all text on equal footing, many of which involve the comparatively simple ideas of substitution or removal. They are, however, no less important to the overall process. These include:   \n",
    "\n",
    "* set all characters to lowercase\n",
    "* remove punctuation (generally part of tokenization, but still worth keeping in mind at this stage, even as confirmation)\n",
    "* remove numbers (or convert numbers to textual representations)\n",
    "* strip white space (also generally part of tokenization)\n",
    "* remove default stop words (general English stop words)\n",
    "\n",
    "## Import Data\n",
    "\n",
    "I've included an excerpt from [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?datasetId=18) in the Data Folder as well! This file is called `Amazon Reviews.csv`.   \n",
    "\n",
    "I have reduced it into a smaller one called `Food_Review.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Food_Review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[jupyter and pandas display](http://songhuiming.github.io/pages/2017/04/02/jupyter-and-pandas-display/) is a good resource to help use jupyters display with pandas to the fullest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for automatic linebreaks and multi-line cells.\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress all warnings with this\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking a Large String Into Smaller Strings\n",
    "\n",
    "A big task for preparing string data is breaking the string into smaller substrings. In ths notebook we'll focus on breaking our [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?datasetId=18) excerpt into individual words, then we'll look into trying to make individual sentences. Our goal by the end of this notebook is to be able to take in our excerpt and return a word count pandas dataframe.\n",
    "\n",
    "### Individual Words\n",
    "`str.split()`.    \n",
    "\n",
    "The `split` function inherent to all `str` objects in python allows you to take a string and break it into a list of substrings based on the input it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].head(2).str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want words, let's first lower ervery word in our dataframe.  \n",
    "this is accomplished by using `.str.lower()`\n",
    "\n",
    "The `str.lower()` method will take all `A-Z` characters in the string and turn them into their corresponding `a-z` form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"THE Ohio State University\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We lower all srings \n",
    "df['Text_clean'] = df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_clean'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`str.replace()`\n",
    "We can replace any specified substring within a string with another specified substring using `str.replace()`. This can help us eliminate the pesky punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some substrings we'll want to remove are:\n",
    "## , \",\", \".\", \"!\", \"?\", \"\\'\", '\\\"', \"-\", \"(\", \")\"\n",
    "\n",
    "df['Text_cleaned'] = df['Text_clean'].replace(\",\",\"\")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace(\".\",\"\")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace(\"!\",\"\")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace(\"?\",\"\")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace(\"\\'\",\"\")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace('\\\"',\"\")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace(\"-\",\" \")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace(\"(\",\"\")\n",
    "df['Text_cleaned'] = df['Text_cleaned'].replace(\")\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we clean the content by removing all the  punctuation, \n",
    "df['Text_clean'] = df['Text_clean'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Text_clean'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To convert Digit into numbers   \n",
    "Import `re` library, make sure your column is of type `string`, and use `(?<!\\S)\\d+(?!\\S)` to match sequences of digits that are between start/end of string and whitespace chars. If you want to only match whole entries that are all digits, you may use `^\\d+$` regex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    return num2words(row['Text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import num2words\n",
    "import inflect\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we clean the content by removing all the  numbers \n",
    "df['Text_nonumber'] = df['Text_clean'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Here we clean the content  convert Digit into numbers \n",
    "df['Text_convnumber'] = df.iloc[:,3].astype(str).apply(lambda row: re.sub(r'(^\\d+$)', lambda x: p.number_to_words(x.group()), row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['Text_convnumber'] = df['Text_clean'].apply(num2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picked some arbitrary rows to review.\n",
    "df[['Text_clean','Text_nonumber']][16:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_clean'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we clean the content by removing all the  white space, \n",
    "df['Text_clean'] = df['Text_clean'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_clean'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = df.Text_clean.str.strip().str.split('[\\W_]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\n",
    "\n",
    "# picked some arbitrary rows to review.\n",
    "df[['Text_clean','words']][16:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Word Counts\n",
    "Now that we have a list of the words used in the text we can write a quick loop to make a word count dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = df['Text_clean'].tolist()\n",
    "raw_text = ''.join(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = raw_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We'll make a temporary dictionary to hold the words\n",
    "### Dictionaries are quite useful for word counts\n",
    "word_dict = {}\n",
    "\n",
    "## For each word in the text\n",
    "for word in all_words:\n",
    "    # if the word wasn't already in the dictionary\n",
    "    if word not in word_dict.keys():\n",
    "        # add it\n",
    "        word_dict[word] = 1\n",
    "    # otherwise\n",
    "    else:\n",
    "        # add 1 to the existing count\n",
    "        word_dict[word] = word_dict[word] + 1\n",
    "        \n",
    "## NOTE In the future we could write this as a function\n",
    "## then anytime we want a word count we just need to call the\n",
    "## function!\n",
    "\n",
    "\n",
    "# Let's examine the dictionary\n",
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make the dataframe\n",
    "# Note .count() is a native method for a dataframe object\n",
    "# this is why I used times_used instead!\n",
    "pa_word_counts = pd.DataFrame({'word':list(word_dict.keys()),\n",
    "                               'times_used':list(word_dict.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_word_counts.sort_values('times_used',ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "As a note, you might think it's silly that we care about how many times the word `the` is used. Hold onto that thought for the next notebook(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "Okay I've been talking a lot, now is your time to practice. I've included an excerpt from [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv) in the Data Folder as well! This file is called `IMDB Dataset.csv`.   \n",
    "\n",
    "I have reduced it into a smaller one called `Movie_Review.csv`\n",
    "\n",
    "You're job is to produce a word count dataframe using what we learned above. This should take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Limitations of Built-In `str` Methods\n",
    "Okay so we've seen how useful of the box str methods can be, but as was the case with punctuation clean up, they have their weaknesses as well.\n",
    "\n",
    "For another example of why we might want fancier tools we'll do another quick practice.\n",
    "\n",
    "Try to take the excerpt of Harry Potter and the Prisoner of Azkaban and break it into unique sentences. Let's take 5 minutes on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What Happened?  \n",
    "* What are some issues you ran into?   \n",
    "\n",
    "## Conclusions\n",
    "While some of you probably were already quite familiar with using str methods, it's good to review. Sometimes when cleaning data you'll want something quick and easy to code, and using some of the techniques we'll learn in the following notebooks may be a bit of overkill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
