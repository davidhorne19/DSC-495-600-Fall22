{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK :: Natural Language Toolkit](NLTK-::-Natural-Language-Toolkit)\n",
    "\n",
    "1. [Introducing `nltk`](Introducing-`nltk`)        \n",
    "2. [Tokenizing Strings](#Tokenizing-Strings)    \n",
    "3. [Stop Words](#Stop-Words)       \n",
    "4. [A Warning](#A-Warning)   \n",
    "5. [N-grams](#N-gramss)    \n",
    "\n",
    "\n",
    "\n",
    "# NLTK :: Natural Language Toolkit\n",
    "\n",
    "## Introducing `nltk`\n",
    "Another life saver for prepping your NLP data is the nltk package. `nltk` stands for Natural Language Toolkit and the corresponding documentation can be found here, https://www.nltk.org/.\n",
    "\n",
    "We'll be using this package a decent amount in the program so be sure to get familiar with it.\n",
    "\n",
    "In this notebook we'll see how useful it is for breaking strings into individual substrings (think words or sentences) called tokens. We'll also learn about stopwords and ngrams.\n",
    "\n",
    "`nltk` can be used for more than these three purposes, but we won't introduce those unless we need them later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Food_Review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Summary','Text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We lower all srings \n",
    "df['Text_clean'] = df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Strings\n",
    "Recall that the endgoal of Preprocessing - Cleaning Data, and an important step in that process was to clean our data and made it easier to work with. As a part of that step we had to:\n",
    "\n",
    "* Turn all the words to lowercase (not always necessary, but we've already done that)\n",
    "* remove punctuation (not always necessary)\n",
    "* remove numbers \n",
    "* strip white space (also generally part of tokenization)\n",
    "You can also write a `split` statement.  \n",
    "That process can be called word tokenization, which is the process of breaking strings down into smaller units (in this case words).\n",
    "\n",
    "`nltk` has a number of built-in tokenizer objects that can make this process as simple as a single line of code. Let's check out an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code chunk to ensure that you have\n",
    "# nltk installed properly\n",
    "import nltk\n",
    "\n",
    "# Note your version may be different than mine\n",
    "# That's fine\n",
    "# At the time of writing this notebook I have verson 3.4.5\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You may get an error or two running the code below if you've never run `nltk` before, that's probably because you have to download some data that isn't automatically downloaded with the `nltk` package. I believe they have pretty good error messages that tell you what to do to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our first tokenizer will be\n",
    "## word_tokenize\n",
    "df['tokenized_text'] = df['Text_clean'].apply(nltk.word_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practice\n",
    "## Run the tokenizer on this #FakeTweet\n",
    "fake_tweet = \"tokenizing is really ez-pz :P :D #nlp\"\n",
    "\n",
    "nltk.word_tokenize(fake_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one we have to import from the tokenize subpackage\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Now we make a Tokenizer object\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call the 'tokenize' method of the tokenizer\n",
    "tweet_tokenizer.tokenize(fake_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when it comes to tokenizing strings it really depends on your use case. You can learn more about existing Tokenizer objects here, https://www.nltk.org/api/nltk.tokenize.html, and even learn how to write your own Tokenizer object.\n",
    "\n",
    "For the most part we'll stick to the word_tokenizer, I'll point out when we depart from that norm.\n",
    "\n",
    "### Practice\n",
    "1. Make a list of the tokens in the Food Review data. Don't bother cleaning out punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google how to tokenize sentences using `nltk`. Once you've figured out how to do that:   \n",
    "\n",
    "    A. Create a pandas DataFrame containing the unique sentences from the Goblet of Fire excerpt.   \n",
    "    B. Create a column in your dataframe that contains the tokenized version of the sentence.   \n",
    "https://www.guru99.com/tokenize-words-sentences-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Think of the words you use most often. There are many words in the world that are necessary to form coherent sentences. However, most of those words are not necessary to convey the meaning behind your sentences.\n",
    "\n",
    "That's essentially the idea behind _stop words_. These are frequently used words that can be thought of as \"noise\" for the sake of data analysis. As such it may be useful to remove them prior to analysis. `nltk` stores a corpus (collection of texts) of stopwords in a variety of languages for easy out of the box use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords are stored in the corpus subpackage\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words can be accessed like so\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "1. Remove the stop words from your tokens for the Food Review excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a column in your DataFrame that removes the stopwords from tokenized sentence column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Warning\n",
    "When working on your NLP projects be wary about removing the stopwords. If you've seen The Office, you know our friend Kevin had to go back to using all the words in order for people to understand him.\n",
    "\n",
    "One concern I have with `nltk`'s stopwords is that words like \"no\", \"not\" and \"nor\" on there. The absence of those words can greatly alter the meaning of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I do not like ice cream.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With stop words.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without stop words.\")\n",
    "print([token for token in tokens if token not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "One final data cleaning step we'll discuss here is the creation of n-grams.\n",
    "\n",
    "As you might imagine, just looking at the words used in a piece of text is not always enough to create useful applications. This is because breaking a text up into the unique words is essentially assuming that every piece of text (from here on referred to as a document) is created by randomly pulling words from a bag, which is why this technique is called __Bag of Words__ (more on this next week).\n",
    "\n",
    "In this assumption you lose the information contained in the document's author's word orderings. A step up from simple bag of words is to look at the unique sequence of n words in a row (otherwise known as n-grams).\n",
    "\n",
    "For example, the bigrams (2-grams) for this sentence:\n",
    "\n",
    "`\"I do not like ice cream\"`\n",
    "\n",
    "are\n",
    "\n",
    "`[(\"I\", \"do\"), (\"do\", \"not\"), (\"not\", \"like\"), (\"like\", \"ice\"), (\"ice\", \"cream\")]`.\n",
    "\n",
    "`nltk` also offers functions that take in a list of the tokens (must be in the order they appeared in the text) and outputs an iterator object of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.bigrams makes the bigrams\n",
    "# it returns an iterator object\n",
    "nltk.bigrams(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can turn that into a list like so\n",
    "print(list(nltk.bigrams(nltk.word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.ngrams can make any kind of ngram\n",
    "nltk.ngrams(nltk.word_tokenize(sentence),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(nltk.ngrams(nltk.word_tokenize(sentence),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "1. Produce a list of the 4-grams of the Goblet of Fire Excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a column in your dataframe that contains the bigrams for each sentence of the Goblet of Fire Excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great!\n",
    "We now know enough to move onto our first NLP projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
